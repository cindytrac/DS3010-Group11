{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cindytrac/DS3010-Group11/blob/main/Copy_of_CaseStudy3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3AaPphVQtfQ"
      },
      "source": [
        "# Case Study 3 : Textual analysis of movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xFcGCJQtfS"
      },
      "source": [
        "**Due Date: February 22, 2020, BEFORE the beginning of class at 2:00pm ET**\n",
        "\n",
        "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W96Ki3enQtfT"
      },
      "source": [
        "<img src=\"https://getthematic.com/wp-content/uploads/2018/03/Harris-Word-Cloud-e1522406279125.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z6U9nIKQtfT"
      },
      "source": [
        "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
        "\n",
        "    Aruzhan\n",
        "    \n",
        "    Cindy\n",
        "    \n",
        "    Tiffany"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeewMOzEQtfT"
      },
      "source": [
        "**Desired outcome of the case study.**\n",
        "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
        "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
        "    * It contains written reviews of movies divided into positive and negative reviews.\n",
        "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
        "    \n",
        "**Required Readings:** \n",
        "* This case study will be based upon the scikit-learn Python library\n",
        "* We will build upon the tutorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "* In particular, this case study is quite similar to \"Exercise 2: Sentiment Analysis on movie reviews\" on the above web page.\n",
        "* Read about deep learning at https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
        "\n",
        "\n",
        "**Case study assumptions:**\n",
        "* You have access to a python installation\n",
        "\n",
        "**Required Python libraries:**\n",
        "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
        "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
        "* Scikit-learn (scikit-learn.org).\n",
        "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
        "\n",
        "** NOTE **\n",
        "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data onto Colab example."
      ],
      "metadata": {
        "id": "WQQrXatF30aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ludq2Spg3zdC",
        "outputId": "a3c2eb66-aee7-477f-8888-40d4bab82dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-20 17:23:04--  https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-20 17:23:04 (24.9 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look for the directory txt_sentoken"
      ],
      "metadata": {
        "id": "j6lyvK0T4HCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! tar xzf review_polarity.tar.gz\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqAfVpEJ4A0P",
        "outputId": "56a31d56-0a52-473e-f698-927002525e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exercise_02_sentiment.py  poldata.README.2.0\t  sample_data\n",
            "fetch_data.py\t\t  review_polarity.tar.gz  txt_sentoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up Libraries"
      ],
      "metadata": {
        "id": "l_YBUeBqF6DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "gYuOrMyVF5l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m-jzLoyQtfU"
      },
      "source": [
        "## Problem 1 (10 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyZeq4COQtfU"
      },
      "source": [
        "* Installing scikit-learn using Anaconda does not necessarily download the example source-code.\n",
        "* Accordingly, you may need to download these directly from Github at https://github.com/scikit-learn/scikit-learn:\n",
        "    * The data can be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
        "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
        "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
        "* Here is a direct link to the code to help you out:  https://github.com/scikit-learn/scikit-learn/tree/main/doc/tutorial/text_analytics\n",
        "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
        "\n",
        "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
        "* This will likely involve moving around data files and/or small modifications to the script."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up Scikit Exercise 2 Data"
      ],
      "metadata": {
        "id": "k-O4uPE5GKBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
        "%run /content/fetch_data.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "1Z3wgf-jZlOx",
        "outputId": "33d994e8-ef89-4689-93bf-12f1b78cdae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-20 17:23:10--  https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 745 [text/plain]\n",
            "Saving to: ‘fetch_data.py.1’\n",
            "\n",
            "\rfetch_data.py.1       0%[                    ]       0  --.-KB/s               \rfetch_data.py.1     100%[===================>]     745  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-20 17:23:10 (33.8 MB/s) - ‘fetch_data.py.1’ saved [745/745]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmoviedir = r'D:\\\\Lab\\nltk_data\\\\corpora\\\\movie_reviews'\\nmovie = load_files(moviedir, shuffle=True)\\nlen(movie.data)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Scikit Exercise 2 Solution Code"
      ],
      "metadata": {
        "id": "EnzTmQJ1K19B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCfgz-eLKoX1",
        "outputId": "871ae0e3-5be5-4333-f0c1-b5d08655f200"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-20 17:26:07--  https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3140 (3.1K) [text/plain]\n",
            "Saving to: ‘exercise_02_sentiment.py’\n",
            "\n",
            "\rexercise_02_sentime   0%[                    ]       0  --.-KB/s               \rexercise_02_sentime 100%[===================>]   3.07K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-02-20 17:26:07 (53.2 MB/s) - ‘exercise_02_sentiment.py’ saved [3140/3140]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXAMPLE CODE TAKEN FROM SCIKIT \n",
        "# Our team used this code as a starting point\n",
        "\n",
        "\"\"\"Build a sentiment analysis / polarity model\n",
        "\n",
        "Sentiment analysis can be casted as a binary text classification problem,\n",
        "that is fitting a linear classifier on features extracted from the text\n",
        "of the user messages so as to guess whether the opinion of the author is\n",
        "positive or negative.\n",
        "\n",
        "In this examples we will use a movie review dataset.\n",
        "\n",
        "\"\"\"\n",
        "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
        "# License: Simplified BSD\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
        "    # block to be able to use a multi-core grid search that also works under\n",
        "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
        "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
        "    # that is used when n_jobs != 1 in GridSearchCV\n",
        "\n",
        "    # the training data folder must be passed as first argument\n",
        "    movie_reviews_data_folder = '/content/txt_sentoken'\n",
        "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
        "    print(\"n_samples: %d\" % len(dataset.data))\n",
        "\n",
        "    # split the dataset in training and test set:\n",
        "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "\n",
        "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
        "    # that are too rare or too frequent\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "        ('clf', LinearSVC(C=1000)),\n",
        "    ])\n",
        "\n",
        "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
        "    # more useful.\n",
        "    # Fit the pipeline on the training set using grid search for the parameters\n",
        "    parameters = {\n",
        "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "    }\n",
        "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
        "    grid_search.fit(docs_train, y_train)\n",
        "\n",
        "    # TASK: print the mean and std for each candidate along with the parameter\n",
        "    # settings for all the candidates explored by grid search.\n",
        "    n_candidates = len(grid_search.cv_results_['params'])\n",
        "    for i in range(n_candidates):\n",
        "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
        "                 % (grid_search.cv_results_['params'][i],\n",
        "                    grid_search.cv_results_['mean_test_score'][i],\n",
        "                    grid_search.cv_results_['std_test_score'][i]))\n",
        "\n",
        "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
        "    # named y_predicted\n",
        "    y_predicted = grid_search.predict(docs_test)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(metrics.classification_report(y_test, y_predicted,\n",
        "                                        target_names=dataset.target_names))\n",
        "\n",
        "    # Print and plot the confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "    print(cm)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.matshow(cm)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "ZznU2cjvLAvk",
        "outputId": "c1a3d2ac-2fd1-40a5-f6cb-f3a6186633a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples: 2000\n",
            "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.84; std - 0.01\n",
            "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.86; std - 0.02\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.85      0.84       242\n",
            "         pos       0.85      0.84      0.85       258\n",
            "\n",
            "    accuracy                           0.84       500\n",
            "   macro avg       0.84      0.84      0.84       500\n",
            "weighted avg       0.84      0.84      0.84       500\n",
            "\n",
            "[[205  37]\n",
            " [ 41 217]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cVyJKHQtfV"
      },
      "source": [
        "## Problem 2 (10 points): Explore the scikit-learn TfidVectorizer class\n",
        "\n",
        "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.** \n",
        "* Define the term frequency–inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
        "* Run the TfidVectorizer class on the training data above (docs_train).\n",
        "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
        "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***a. Term Frequency–Inverse Document Frequency (TF-IDF)*** \n",
        "\n",
        "> **TF-IDF** is a quantitative statistic that reflects the importance of a word within a collection. TF-IDF computes a score for each word. The weight of each term is proportional to it's frequency (how many times the word appears in the text). This technique is often used in the fields of text mining and information retrieval.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l2_ImzH_o7KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***b. min_df and max_df of TfidVectorizer***\n",
        "\n",
        ">  You can use **min_df** to remove terms that appear too infrequently whereas you can use **max_df** to remove terms that appear too frequently. The given threshold is called corpus-specific stop words and the parameter is a proportion of the docs. The default for both min_df and max_df is 1, meaning it ignores terms that appear in less/more than 1 doc (does not ignore any terms). \n",
        "\n",
        " >>**min_df** example:\n",
        "\n",
        ">>*   min_df = 0.02 → ignore terms that appear in LESS than 2% of the documents\n",
        "\n",
        ">>**max_df** example: \n",
        "\n",
        ">>*   max_df= 0.25 → ignore terms that appear in MORE than 25% of the documents\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9fhDS3biMql2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***c. ngram_range of TfidVectorizer***\n",
        "\n",
        ">  For different extracted n-grams, **ngram_range** sets the lower and upper boundaries of the range of n-values. \n",
        "\n",
        ">>**ngram_range** example:\n",
        "ngram_range = (1,2) → unigrams and bigrams\n",
        ">>*   large ngram_range values may take a very long time to run"
      ],
      "metadata": {
        "id": "qgiN5W5FTI0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the training data folder must be passed as first argument\n",
        "movie_reviews_data_folder = '/content/txt_sentoken'\n",
        "dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
        "print(\"n_samples: %d\" % len(dataset.data))\n",
        "\n",
        "# list of category names\n",
        "print(dataset.target_names)\n",
        "\n",
        "# files are loaded in memory in the data attribute\n",
        "#print(len(dataset.data))\n",
        "#print(len(dataset.filenames))\n",
        "\n",
        "# tokenizing text with scikit-learn -----------------------------------------\n",
        "# using docs_train\n",
        "docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(docs_train)\n",
        "X_train_counts.shape\n",
        "print(\"Number of words? =\", X_train_counts.shape[1])\n",
        "\n",
        "# converting from occurrences to frequencies\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# fit estimator to data\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
        "\n",
        "# transform  count-matrix to a tf-idf representation\n",
        "X_train_tf = tf_transformer.transform(X_train_counts)\n",
        "X_train_tf.shape\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03LVOBloO0au",
        "outputId": "a83de9b0-d44f-425c-c1d9-5ee65eab17a3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples: 2000\n",
            "['neg', 'pos']\n",
            "Number of words? = 39659\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 39659)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YGCOW1yjQtfW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "95e9287b-3872-4ff3-afa9-7c0c1e495705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 9)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-af794471e601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m docs_train, docs_test, y_train, y_test = train_test_split(movie.data, movie.target, \n\u001b[0m\u001b[1;32m     19\u001b[0m                                                           test_size = 0.20, random_state = 12)\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'movie' is not defined"
          ]
        }
      ],
      "source": [
        "# aru code?\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import load_files\n",
        "\n",
        "corpus = [\n",
        "          'This is the first document.',\n",
        "          'This document is the second document.',\n",
        "          'And this is the third one.',\n",
        "          'Is this the first document?',\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "vectorizer.get_feature_names_out()\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "docs_train, docs_test, y_train, y_test = train_test_split(movie.data, movie.target, \n",
        "                                                          test_size = 0.20, random_state = 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWQAJmsQtfW"
      },
      "source": [
        "## Problem 3 (15 points): Machine learning algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNt6Ue6-QtfW"
      },
      "source": [
        "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
        "    * \"fit\" your TfidfVectorizer using docs_train\n",
        "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
        "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
        "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
        "* Examine two classifiers provided by scikit-learn \n",
        "    * LinearSVC\n",
        "    * KNeighborsClassifier\n",
        "    * Why do you think it might be working better?\n",
        "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
        "    * Can you conjecture on why the classifier made a mistake for this prediction?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"fit\" your TfidfVectorizer using docs_train"
      ],
      "metadata": {
        "id": "iZXui2MwR3Um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting using docs_train\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from scipy.sparse.csr import csr_matrix\n",
        "\n",
        "# dataset.target\n",
        "\n",
        "#instantiate CountVectorizer() \n",
        "count_vect_train = CountVectorizer()\n",
        "X_counts = count_vect_train.fit_transform(dataset.data)\n",
        "y_counts = dataset.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_counts, y_counts, test_size = 0.20, random_state = 12)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "# # this steps generates word counts for the words in your docs \n",
        "# X_train_counts = count_vect_train.fit_transform(X_train)\n",
        "# X_test_counts = count_vect_train.fit_transform(X_test)\n",
        "# X_train_counts.shape\n",
        "\n",
        "# print(\"Number of words =\", X_train_counts.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ7XjHnnCAjZ",
        "outputId": "dadb3a03-ba6a-4fa8-9eac-16c3bf136062"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600, 39659)\n",
            "(400, 39659)\n",
            "(1600,)\n",
            "(400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs_train))\n",
        "print(len(docs_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhWU2431b0xA",
        "outputId": "d5756457-c66a-41bf-d24d-82010270f730"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n"
      ],
      "metadata": {
        "id": "5mWqt4YN6byu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "collapsed": true,
        "id": "m945iZIxQtfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "253bc866-6cc6-4af6-f629-6c43be40f515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600, 39659)\n",
            "  (0, 49)\t0.008719751330189448\n",
            "  (0, 161)\t0.008719751330189448\n",
            "  (0, 172)\t0.008719751330189448\n",
            "  (0, 183)\t0.008719751330189448\n",
            "  (0, 185)\t0.008719751330189448\n",
            "  (0, 186)\t0.008719751330189448\n",
            "  (0, 205)\t0.026159253990568343\n",
            "  (0, 800)\t0.008719751330189448\n",
            "  (0, 865)\t0.008719751330189448\n",
            "  (0, 915)\t0.008719751330189448\n",
            "  (0, 966)\t0.008719751330189448\n",
            "  (0, 999)\t0.008719751330189448\n",
            "  (0, 1027)\t0.008719751330189448\n",
            "  (0, 1094)\t0.008719751330189448\n",
            "  (0, 1256)\t0.017439502660378896\n",
            "  (0, 1268)\t0.017439502660378896\n",
            "  (0, 1271)\t0.008719751330189448\n",
            "  (0, 1275)\t0.008719751330189448\n",
            "  (0, 1287)\t0.008719751330189448\n",
            "  (0, 1502)\t0.017439502660378896\n",
            "  (0, 1559)\t0.008719751330189448\n",
            "  (0, 1579)\t0.026159253990568343\n",
            "  (0, 1599)\t0.017439502660378896\n",
            "  (0, 1686)\t0.017439502660378896\n",
            "  (0, 1718)\t0.043598756650947236\n",
            "  :\t:\n",
            "  (1599, 37372)\t0.02520563459323681\n",
            "  (1599, 37514)\t0.02520563459323681\n",
            "  (1599, 37720)\t0.05041126918647362\n",
            "  (1599, 37843)\t0.02520563459323681\n",
            "  (1599, 38019)\t0.02520563459323681\n",
            "  (1599, 38059)\t0.02520563459323681\n",
            "  (1599, 38083)\t0.02520563459323681\n",
            "  (1599, 38089)\t0.02520563459323681\n",
            "  (1599, 38405)\t0.10082253837294725\n",
            "  (1599, 38413)\t0.02520563459323681\n",
            "  (1599, 38426)\t0.02520563459323681\n",
            "  (1599, 38494)\t0.02520563459323681\n",
            "  (1599, 38571)\t0.05041126918647362\n",
            "  (1599, 38632)\t0.02520563459323681\n",
            "  (1599, 38678)\t0.02520563459323681\n",
            "  (1599, 38711)\t0.12602817296618407\n",
            "  (1599, 38715)\t0.02520563459323681\n",
            "  (1599, 38781)\t0.07561690377971043\n",
            "  (1599, 39002)\t0.02520563459323681\n",
            "  (1599, 39013)\t0.1764394421526577\n",
            "  (1599, 39096)\t0.15123380755942087\n",
            "  (1599, 39143)\t0.02520563459323681\n",
            "  (1599, 39196)\t0.02520563459323681\n",
            "  (1599, 39203)\t0.02520563459323681\n",
            "  (1599, 39396)\t0.02520563459323681\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "# Initialize the parameters of the vectorizer\n",
        "vectorizer = TfidfVectorizer(input=docs_train, analyzer='word', ngram_range=(1,1),\n",
        "                     min_df = 0, stop_words=None)\n",
        "\n",
        "X = vectorizer.fit_transform(docs_train)\n",
        "vectorizer.get_feature_names_out()\n",
        "X.shape\n",
        "'''\n",
        "\n",
        "# fit estimator to data\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train)\n",
        "\n",
        "# transform count-matrix to a tf-idf representation\n",
        "X_train_tf = tf_transformer.transform(X_train)\n",
        "print(X_train_tf.shape)\n",
        "print(X_train_tf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test"
      ],
      "metadata": {
        "id": "LNwvaHW8rlF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# converting from occurrences to frequencies\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "# fit estimator to data\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_test)\n",
        "\n",
        "# transform count-matrix to a tf-idf representation\n",
        "X_test_tf = tf_transformer.transform(X_test)\n",
        "print(X_test_tf.shape)\n",
        "print(X_test_tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMRBFcc5rmA5",
        "outputId": "b30f0f9a-1f26-4035-8511-3262369a3561"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400, 39659)\n",
            "  (0, 211)\t0.015410168209838634\n",
            "  (0, 212)\t0.015410168209838634\n",
            "  (0, 502)\t0.06164067283935454\n",
            "  (0, 556)\t0.0924610092590318\n",
            "  (0, 605)\t0.015410168209838634\n",
            "  (0, 638)\t0.015410168209838634\n",
            "  (0, 750)\t0.03082033641967727\n",
            "  (0, 826)\t0.015410168209838634\n",
            "  (0, 966)\t0.03082033641967727\n",
            "  (0, 967)\t0.0924610092590318\n",
            "  (0, 982)\t0.015410168209838634\n",
            "  (0, 992)\t0.03082033641967727\n",
            "  (0, 1256)\t0.015410168209838634\n",
            "  (0, 1283)\t0.015410168209838634\n",
            "  (0, 1301)\t0.015410168209838634\n",
            "  (0, 1502)\t0.0462305046295159\n",
            "  (0, 1562)\t0.015410168209838634\n",
            "  (0, 1577)\t0.015410168209838634\n",
            "  (0, 1718)\t0.015410168209838634\n",
            "  (0, 1726)\t0.015410168209838634\n",
            "  (0, 1738)\t0.015410168209838634\n",
            "  (0, 1755)\t0.015410168209838634\n",
            "  (0, 1760)\t0.06164067283935454\n",
            "  (0, 1810)\t0.21574235493774088\n",
            "  (0, 1855)\t0.015410168209838634\n",
            "  :\t:\n",
            "  (399, 38133)\t0.013863506393873152\n",
            "  (399, 38347)\t0.013863506393873152\n",
            "  (399, 38405)\t0.16636207672647782\n",
            "  (399, 38469)\t0.013863506393873152\n",
            "  (399, 38475)\t0.013863506393873152\n",
            "  (399, 38504)\t0.013863506393873152\n",
            "  (399, 38610)\t0.013863506393873152\n",
            "  (399, 38632)\t0.041590519181619455\n",
            "  (399, 38707)\t0.013863506393873152\n",
            "  (399, 38711)\t0.05545402557549261\n",
            "  (399, 38715)\t0.027727012787746304\n",
            "  (399, 38781)\t0.041590519181619455\n",
            "  (399, 38811)\t0.013863506393873152\n",
            "  (399, 38849)\t0.013863506393873152\n",
            "  (399, 38986)\t0.013863506393873152\n",
            "  (399, 39013)\t0.027727012787746304\n",
            "  (399, 39026)\t0.013863506393873152\n",
            "  (399, 39069)\t0.027727012787746304\n",
            "  (399, 39131)\t0.027727012787746304\n",
            "  (399, 39165)\t0.013863506393873152\n",
            "  (399, 39203)\t0.08318103836323891\n",
            "  (399, 39279)\t0.013863506393873152\n",
            "  (399, 39281)\t0.013863506393873152\n",
            "  (399, 39396)\t0.013863506393873152\n",
            "  (399, 39475)\t0.013863506393873152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examining LinearSVC and KNeighborsClassifier"
      ],
      "metadata": {
        "id": "YxvDC6t7rmSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "'''\n",
        "docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "'''\n",
        "\n",
        "# USING LINEAR SUPPORT VECTOR MACHINE (SVM)\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "SVC_clf = svm.SVC()\n",
        "\n",
        "print(\"Shape of X_train_tf: \", X_train_tf.shape[0], \", \", X_train_tf.shape[1])\n",
        "print(\"Shape of X_test_tf: \", X_test_tf.shape[0], \", \", X_test_tf.shape[1])\n",
        "\n",
        "SVC_clf.fit(X_train_tf, y_train)\n",
        "y_predicted = SVC_clf.predict(X_test_tf)\n",
        "pred = np.mean(y_predicted == y_test)\n",
        "\n",
        "print(\"Achieved\",(pred*100),\"% accuracy using SVC\")\n",
        "print()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF8j9E5FisoU",
        "outputId": "fdd28653-7f3e-4f49-abd8-b9efcc804f3b"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_tf:  1600 ,  39659\n",
            "Shape of X_test_tf:  400 ,  39659\n",
            "Achieved 80.5 % accuracy using SVC\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "'''\n",
        "docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "'''\n",
        "\n",
        "# USING LINEAR SUPPORT VECTOR MACHINE (SVM)\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf = KNeighborsClassifier(n_neighbors=10)\n",
        "# y_train = np.reshape(y_train, (-1, 1))\n",
        "\n",
        "print(\"Shape of X_train_tf: \", X_train_tf.shape[0], \", \", X_train_tf.shape[1])\n",
        "print(\"Shape of X_test_tf: \", X_test_tf.shape[0], \", \", X_test_tf.shape[1])\n",
        "\n",
        "text_clf.fit(X_train_tf, y_train)\n",
        "y_predicted = text_clf.predict(X_test_tf)\n",
        "pred = np.mean(y_predicted == y_test)\n",
        "\n",
        "print(\"Achieved\",(pred*100),\"% accuracy using KNN\")\n",
        "print()\n",
        "\n",
        "# print(metrics.classification_report(docs_test, predicted,\n",
        "#      target_names=dataset.target_names))\n",
        "\n",
        "# metrics.confusion_matrix(docs_test, predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy2fuks9XZJL",
        "outputId": "d7bba41f-c8c7-4d8b-cddc-c17796248f4a"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_tf:  1600 ,  39659\n",
            "Shape of X_test_tf:  400 ,  39659\n",
            "Achieved 61.25000000000001 % accuracy using KNN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoicR0YeQx5x"
      },
      "source": [
        "## Problem 4 (15 points): Use a Multi-Layer Perceptron (MLP) for classifying the reviews.  Explore the parameters for the MLP and compare the accuracies against your baseline algorithms in Problem 1.\n",
        "\n",
        "**Read the documentation for the MLPClassifier class at https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier.** \n",
        "* Note: This is *very similar* to using the LinearSVC and KNeighborsClassifier classes above!\n",
        "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of accuracy?\n",
        "* Try different values for \"activation\". What do you observe in terms of accuracy?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaDYg8XNQx5y"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G65MbRfQx5y"
      },
      "source": [
        "## Problem 5 (10 points): Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
        "**Compare the runtime of your  baseline algorithms to the runtime of the MLPClassifier** \n",
        "\n",
        "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
        "* Try different values for \"hidden_layer_sizes\".  What do you observe in terms of runtime?\n",
        "* Try different values for \"activation\". What do you observe in terms of runtime?\n",
        "* How long does the \"fit\" function take as opposed to the \"predict\" function?  Can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smz1f8tyQtfX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "o0bcrrUzQtfX"
      },
      "source": [
        "\n",
        "## Problem 6 (20 points): Business question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VZdOxfQtfX"
      },
      "source": [
        "* Suppose you had a machine learning algorithm that could detect the sentiment of tweets that was highly accurate.  What kind of business could you build around that?\n",
        "* Who would be your competitors, and what are their sizes?\n",
        "* What would be the size of the market for your product?\n",
        "* In addition, assume that your machine learning was slow to train, but fast in making predictions on new data.  How would that affect your business plan?\n",
        "* How could you use the cloud to support your product?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The business that could be built with the case of a machine learning algorith detecting tweets accurately is for a online clothing shop. Specifically, one that gathers multiple brands in their store. The shop would need to see brands that have the most accurate reviews and within those results the shop can look at which brands to invest in. With more accurate brands that have the best reviews, the more people would buy from them as there products are high quality and approved by many. \n",
        "\n",
        "2. Other competitors would be other online shops as well as other brands that would want their products to be brought into these shops. It creates external competition with other clothing stores that are doing the same thing as well as internal competitions within brands that want their products to be distruibuted. \n",
        "\n",
        "3. The size of this market is not as large as regular clothing companies that are their own brand themselves. Since this businesss collects many other brands to sell, the market would be mid sized.\n",
        "\n",
        "4.\n",
        "\n",
        "5. "
      ],
      "metadata": {
        "id": "Loyhoq8Sr69d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-g3psMyQtfX"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHQoH_KQtfX"
      },
      "source": [
        "# Slides (for a 5-8 minute presentation) (20 points)\n",
        "\n",
        "\n",
        "1. (5 points) Motivation about the data collection, why the topic is interesting to you. \n",
        "\n",
        "2. (10 points) Communicating Results (figure/table)\n",
        "\n",
        "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl5sy8EUQtfY"
      },
      "source": [
        "\n",
        "# Done\n",
        "\n",
        "All set! \n",
        "\n",
        "** What do you need to submit?**\n",
        "\n",
        "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
        "\n",
        "\n",
        "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study. \n",
        "\n",
        "* **Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
        "    * What data you collected? \n",
        "    * Why this topic is interesting or important to you? (Motivations)\n",
        "    * How did you analyse the data?\n",
        "    * What did you find in the data? \n",
        " \n",
        "     (please include figures or tables in the report, but no source code)\n",
        "\n",
        "\n",
        "*Please compress all the files into a single zipped file.*\n",
        "\n",
        "\n",
        "** How to submit: **\n",
        "\n",
        "        Please submit through canvas.wpi.edu\n",
        "\n",
        "### DS3010 Case Study 3 Team ??\n",
        "\n",
        "#### where ?? is your team number.\n",
        "        \n",
        "** Note: Each team just needs to submits one submission **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HylAvN1gQtfY"
      },
      "source": [
        "# Grading Criteria:\n",
        "\n",
        "**Total Points: 100**\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Notebook results:**\n",
        "    Points: 80\n",
        "\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 1:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "    \n",
        "    -----------------------------------\n",
        "    Question 2:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "        \n",
        "    -----------------------------------\n",
        "    Question 3:\n",
        "    Points: 15 \n",
        "    -----------------------------------\n",
        "  \n",
        "    -----------------------------------\n",
        "    Question 4:  \n",
        "    Points: 15\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 5:  \n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 6:  \n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Slides (for a 5-8 minute presentation): Story-telling**\n",
        "    Points: 20\n",
        "\n",
        "\n",
        "1. Motivation about the data collection, why the topic is interesting to you.\n",
        "    Points: 5 \n",
        "\n",
        "2. Communicating Results (figure/table)\n",
        "    Points: 10 \n",
        "\n",
        "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
        "    Points: 5 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch582GFeQtfY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.2.0"
    },
    "colab": {
      "name": "Copy of CaseStudy3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}